---
created: 2026-02-22
updated: 2026-02-22
tags: [supercomputing, distributed-training]
domain: supercomputing
status: seed
---

# Data Parallelism

## Summary
Replicate model shards across devices and synchronize gradients efficiently.

## Connections
- [[Tensor and Pipeline Parallelism]]
- [[GPU Cluster Architecture]]

