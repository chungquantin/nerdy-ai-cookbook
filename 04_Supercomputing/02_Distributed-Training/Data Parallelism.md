---
created: 2026-02-22
updated: 2026-02-22
tags: [supercomputing, distributed-training]
domain: supercomputing
status: seed
---

# Data Parallelism

## Summary
Replicate model shards across devices and synchronize gradients efficiently.

## Connections
- [[Tensor and Pipeline Parallelism]]
- [[GPU Cluster Architecture]]
- [[04_Supercomputing/04_Knowledge/data-parallelism/README]]

