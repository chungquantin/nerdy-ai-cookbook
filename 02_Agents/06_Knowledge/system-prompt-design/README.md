---
created: 2026-02-22
updated: 2026-02-22
tags: [agents, knowledge, beginner]
domain: agents
status: active
source_url: "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview"
kind: topic
---

# System Prompt Design

## Source
Original Source: [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)
Captured on 2026-02-22.

## Abstract
Claude API Documentation This dossier treats the material as a beginner-level knowledge and cross-checks it against corroborating sources.

## Context and Problem Framing
The source frames the problem around Solutions, Partners, Learn, Company. If not, we highly suggest you spend time establishing that first. Check out Define success criteria and build evaluations for tips and guidance.

## Technical Approach
The synthesis compares claims across the primary source and additional references, then normalizes them into a systems-level narrative. Try the prompt generator in the Claude Console! This guide focuses on success criteria that are controllable through prompt engineering. Not every success criteria or failing eval is best solved by prompt engineering.

## Main Findings
While these tips apply broadly to all Claude models, you can find prompting tips specific to extended thinking models here. If not, we highly suggest you spend time establishing that first. Check out Define success criteria and build evaluations for tips and guidance. For example, latency and cost can be sometimes more easily improved by selecting a different model. The prompt engineering pages in this section have been organized from most broadly effective techniques to more specialized techniques.

## Critical Analysis
When troubleshooting performance, we suggest you try these techniques in order, although the actual impact of each technique will depend on your use case. If you're an interactive learner, you can dive into our interactive tutorials instead! Conflicting assumptions across sources should be resolved through targeted experiments before production adoption.

## Application to This Cookbook
In this cookbook, this dossier should be connected to [[02_Agents/Agents Index]], [[01_MOCs/Agents MOC]], and [[02_Agents/06_Knowledge/Knowledge Index]]. Use this note as a foundation for implementation logs, benchmark deltas, and architecture decisions.

## Graph Connections
- [[02_Agents/Agents Index]]
- [[01_MOCs/Agents MOC]]
- [[02_Agents/06_Knowledge/Knowledge Index]]

## Bibliography
- [Prompt engineering overview - Claude API Docs](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)
- [Prompt engineering | OpenAI API](https://platform.openai.com/docs/guides/prompt-engineering)
- [Prompt engineering techniques - Azure OpenAI | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering)

## Research Notes
Record implementation evidence, benchmark outcomes, disagreements between sources, and open questions for follow-up.