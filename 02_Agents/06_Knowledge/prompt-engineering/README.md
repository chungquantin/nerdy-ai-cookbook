---
created: 2026-02-22
updated: 2026-02-22
tags: [agents, knowledge, beginner]
domain: agents
status: active
source_url: "https://platform.openai.com/docs/guides/prompt-engineering"
kind: topic
---

# Prompt Engineering

## Source
Original Source: [https://platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering)
Captured on 2026-02-22.

## Abstract
Learn strategies and tactics for better results using large language models in the OpenAI API. This dossier treats the material as a beginner-level knowledge and cross-checks it against corroborating sources.

## Context and Problem Framing
The source frames the problem around Get started, Core concepts, Agents, Tools. Enhance results with prompt engineering strategies. With the OpenAI API, you can use a large language model to generate text from a prompt, as you might using ChatGPT.

## Technical Approach
The synthesis compares claims across the primary source and additional references, then normalizes them into a systems-level narrative. Models can generate almost any kind of text response—like code, mathematical equations, structured JSON data, or human-like prose. Here’s a simple example using the Responses API. An array of content generated by the model is in the output property of the response.

## Main Findings
Learn strategies and tactics for better results using large language models in the OpenAI API. Enhance results with prompt engineering strategies. With the OpenAI API, you can use a large language model to generate text from a prompt, as you might using ChatGPT. The output array often has more than one item in it! It can contain tool calls, data about reasoning tokens generated by reasoning models, and other items.

## Critical Analysis
It is not safe to assume that the model’s text output is present at output[0].content[0].text. Some of our official SDKs include an output_text property on model responses for convenience, which aggregates all text outputs from the model into a single string. Conflicting assumptions across sources should be resolved through targeted experiments before production adoption.

## Application to This Cookbook
In this cookbook, this dossier should be connected to [[02_Agents/Agents Index]], [[01_MOCs/Agents MOC]], and [[02_Agents/06_Knowledge/Knowledge Index]]. Use this note as a foundation for implementation logs, benchmark deltas, and architecture decisions.

## Graph Connections
- [[02_Agents/Agents Index]]
- [[01_MOCs/Agents MOC]]
- [[02_Agents/06_Knowledge/Knowledge Index]]

## Bibliography
- [Prompt engineering | OpenAI API](https://platform.openai.com/docs/guides/prompt-engineering)
- [Prompt engineering overview - Claude API Docs](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)
- [Prompt Engineering Guide | Prompt Engineering Guide<!-- -->](https://www.promptingguide.ai/)

## Research Notes
Record implementation evidence, benchmark outcomes, disagreements between sources, and open questions for follow-up.